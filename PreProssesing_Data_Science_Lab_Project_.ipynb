{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Lab Project"
      ],
      "metadata": {
        "id": "Lavd7LlqU0mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 1:\n",
        "Mounting Google Drive and Loading CSV Files\n",
        "This code block does three main things:\n",
        "\n",
        "Mounts your Google Drive: It connects your Google Drive to the current environment (likely Google Colab) so you can access files stored there. This is done using the drive.mount() function."
      ],
      "metadata": {
        "id": "MpOkfaQaU1Z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5y2UsVvLmSj",
        "outputId": "72870c13-7bea-41d9-fb09-c6edeaefa813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded 2017: (51392, 154)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-155-03358e895310>:16: DtypeWarning: Columns (8,12,13,14,15,16,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dfs[year] = pd.read_csv(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2018: (98855, 129)\n",
            "Loaded 2019: (88883, 85)\n",
            "Loaded 2020: (64461, 61)\n",
            "Loaded 2021: (83439, 48)\n",
            "Loaded 2022: (73268, 79)\n",
            "Loaded 2023: (89184, 84)\n",
            "Loaded 2024: (65437, 114)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Drive & Load All Survey CSVs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import pandas as pd, glob, os, re\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/DataScienceLabProject'\n",
        "csv_paths = (\n",
        "    sorted(glob.glob(os.path.join(DATA_DIR, '[2][0][1-2][7-9].csv'))) +\n",
        "    sorted(glob.glob(os.path.join(DATA_DIR, '202[0-4].csv')))\n",
        ")\n",
        "\n",
        "dfs = {}\n",
        "for path in csv_paths:\n",
        "    year = int(re.search(r'(\\d{4})\\.csv$', path).group(1))\n",
        "    dfs[year] = pd.read_csv(path)\n",
        "    print(f\"Loaded {year}: {dfs[year].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 2:Harmonizing and Standardizing Columns\n",
        "This code block aims to clean and prepare the survey data loaded in Cell 1. It deals with the problem of inconsistent column names across different years of surveys. For instance, 'Salary' in one year might be called 'ConvertedComp' in another. This process ensures we have consistent data for analysis."
      ],
      "metadata": {
        "id": "gAB6YW4EHuzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 ────────────────────────────────────────────────────────────────────\n",
        "# Harmonise columns (advanced aliases) & keep only the 39 relevant features\n",
        "\n",
        "import re, os, numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1)  Define canonical names and ALL their aliases  (edit freely if needed)\n",
        "# -------------------------------------------------------------------------\n",
        "alias_map = {\n",
        "    # 1  Respondent ID\n",
        "    \"respondent_id\" : [\"Respondent\", \"ResponseId\"],\n",
        "\n",
        "    # 2  Country\n",
        "    \"country\"       : [\"Country\"],\n",
        "\n",
        "    # 3  Employment status\n",
        "    \"employment\"    : [\"EmploymentStatus\", \"Employment\"],\n",
        "\n",
        "    # 4  Highest education level\n",
        "    \"education_level\" : [\"FormalEducation\", \"EdLevel\"],\n",
        "\n",
        "    # 5  Organisation size\n",
        "    \"org_size\"      : [\"CompanySize\", \"OrgSize\"],\n",
        "\n",
        "    # 6  Developer role / type\n",
        "    \"dev_type\"      : [\"DeveloperType\", \"DevType\"],\n",
        "\n",
        "    # 7  Total years coding\n",
        "    \"years_code_total\" : [\"YearsProgram\", \"YearsCoding\", \"YearsCode\"],\n",
        "\n",
        "    # 8  Professional years coding\n",
        "    \"years_code_pro\"   : [\"YearsCodedJob\", \"YearsCodingProf\", \"YearsCodePro\"],\n",
        "\n",
        "    # 9  Currency\n",
        "    \"currency\"      : [\"Currency\", \"CurrencySymbol\"],\n",
        "\n",
        "    # 10 Compensation (salary / total comp)\n",
        "    \"compensation_total\" : [\"Salary\", \"ConvertedSalary\",\n",
        "                            \"CompTotal\", \"ConvertedComp\", \"ConvertedCompYearly\"],\n",
        "\n",
        "    # 11–18  Tech stacks (worked / desired)\n",
        "    \"langs_worked\"       : [\"HaveWorkedLanguage\", \"LanguageWorkedWith\",\n",
        "                            \"LanguageHaveWorkedWith\"],\n",
        "    \"langs_desired\"      : [\"WantWorkLanguage\", \"LanguageDesireNextYear\",\n",
        "                            \"LanguageWantToWorkWith\"],\n",
        "\n",
        "    \"db_worked\"          : [\"HaveWorkedDatabase\", \"DatabaseWorkedWith\",\n",
        "                            \"DatabaseHaveWorkedWith\"],\n",
        "    \"db_desired\"         : [\"WantWorkDatabase\", \"DatabaseDesireNextYear\",\n",
        "                            \"DatabaseWantToWorkWith\"],\n",
        "\n",
        "    \"platform_worked\"    : [\"HaveWorkedPlatform\", \"PlatformWorkedWith\",\n",
        "                            \"PlatformHaveWorkedWith\"],\n",
        "    \"platform_desired\"   : [\"WantWorkPlatform\", \"PlatformDesireNextYear\",\n",
        "                            \"PlatformWantToWorkWith\"],\n",
        "\n",
        "    \"webframe_worked\"    : [\"HaveWorkedFramework\", \"FrameworkWorkedWith\",\n",
        "                            \"WebFrameWorkedWith\", \"WebframeWorkedWith\",\n",
        "                            \"WebframeHaveWorkedWith\"],\n",
        "    \"webframe_desired\"   : [\"WantWorkFramework\", \"FrameworkDesireNextYear\",\n",
        "                            \"WebFrameDesireNextYear\", \"WebframeDesireNextYear\",\n",
        "                            \"WebframeWantToWorkWith\"],\n",
        "\n",
        "    # 19 Survey length\n",
        "    \"survey_length\"      : [\"SurveyLong\", \"SurveyTooLong\", \"SurveyLength\"],\n",
        "\n",
        "    # 20 Operating system\n",
        "    \"operating_system\"   : [\"OperatingSystem\", \"OpSys\",\n",
        "                            \"OpSysPersonal use\", \"OpSysProfessional use\"],\n",
        "\n",
        "    # 21–24  Stack Overflow usage\n",
        "    \"so_visit_freq\"  : [\"StackOverflowVisit\", \"SOVisitFreq\"],\n",
        "    \"so_account\"     : [\"StackOverflowHasAccount\", \"SOAccount\"],\n",
        "    \"so_part_freq\"   : [\"StackOverflowParticipate\", \"SOPartFreq\"],\n",
        "    \"so_community\"   : [\"StackOverflowCommunity\", \"SOComm\"],\n",
        "\n",
        "    # 25–28  Demographics\n",
        "    \"age\"            : [\"Age\"],\n",
        "    \"survey_ease\"    : [\"SurveyEasy\", \"SurveyEase\"],\n",
        "    \"gender\"         : [\"Gender\"],\n",
        "    \"ethnicity\"      : [\"Race\", \"RaceEthnicity\", \"Ethnicity\"],\n",
        "\n",
        "    # 29  Main branch\n",
        "    \"main_branch\"    : [\"MainBranch\"],\n",
        "\n",
        "    # 30–31  Misc tech\n",
        "    \"misc_tech_worked\"  : [\"MiscTechWorkedWith\", \"MiscTechHaveWorkedWith\"],\n",
        "    \"misc_tech_desired\" : [\"MiscTechDesireNextYear\", \"MiscTechWantToWorkWith\"],\n",
        "\n",
        "    # 32–33  Satisfaction\n",
        "    \"job_satisfaction\"     : [\"JobSatisfaction\", \"JobSat\"],\n",
        "    \"career_satisfaction\"  : [\"CareerSatisfaction\", \"CareerSat\"],\n",
        "\n",
        "    # 34  Open-source participation\n",
        "    \"open_source\"       : [\"OpenSource\", \"OpenSourcer\"],\n",
        "\n",
        "    # 35  Learning resources\n",
        "    \"learning_resources\": [\"EducationTypes\", \"SelfTaughtTypes\", \"EduOther\",\n",
        "                           \"NEWLearn\", \"LearnCode\", \"LearnCodeOnline\",\n",
        "                           \"LearnCodeCoursesCert\"],\n",
        "\n",
        "    # 36  Time since last job change\n",
        "    \"last_job_change\"   : [\"LastNewJob\", \"LastHireDate\",\n",
        "                           \"NEWJobHunt\", \"NEWOnboardGood\"],\n",
        "\n",
        "    # 37  Job factors importance\n",
        "    \"job_factors\"       : [\"JobFactors\"] + [f\"AssessJob{i}\" for i in range(1, 11)],\n",
        "\n",
        "    # 38  Undergraduate major\n",
        "    \"undergrad_major\"   : [\"MajorUndergrad\", \"UndergradMajor\"],\n",
        "\n",
        "    # 39  Work-life balance\n",
        "    \"work_life_balance\" : [\"ExCoderBalance\", \"BetterLife\"],\n",
        "}\n",
        "\n",
        "# Quick lookup: alias → canonical\n",
        "alias_to_canon = {alias: canon\n",
        "                  for canon, aliases in alias_map.items()\n",
        "                  for alias in aliases}\n",
        "\n",
        "# Also map canonical names to themselves (in case some files already use them)\n",
        "for canon in alias_map.keys():\n",
        "    alias_to_canon.setdefault(canon, canon)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2)  Consolidate all aliases inside each dataframe\n",
        "# -------------------------------------------------------------------------\n",
        "def consolidate_aliases(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Gather actual columns per canonical\n",
        "    present_by_canon = defaultdict(list)\n",
        "    for col in df.columns:\n",
        "        canon = alias_to_canon.get(col)\n",
        "        if canon:\n",
        "            present_by_canon[canon].append(col)\n",
        "\n",
        "    # Merge columns row-wise (first non-NA wins)\n",
        "    for canon, cols in present_by_canon.items():\n",
        "        if len(cols) == 1 and cols[0] == canon:\n",
        "            # Already canonical and unique → nothing to do\n",
        "            continue\n",
        "\n",
        "        merged = df[cols[0]].copy()\n",
        "        for alt in cols[1:]:\n",
        "            merged = merged.combine_first(df[alt])\n",
        "\n",
        "        # If canonical name itself exists separately, merge it too\n",
        "        if canon in df.columns and canon not in cols:\n",
        "            merged = df[canon].combine_first(merged)\n",
        "\n",
        "        df[canon] = merged\n",
        "        # Drop all non-canonical alias cols we just merged\n",
        "        drop = [c for c in cols if c != canon]\n",
        "        df.drop(columns=drop, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "for year in dfs:\n",
        "    dfs[year] = consolidate_aliases(dfs[year])\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3)  Keep ONLY the 39 canonical columns we care about\n",
        "# -------------------------------------------------------------------------\n",
        "CANON_COLS = list(alias_map.keys())   # preserve order for convenience\n",
        "\n",
        "for year, df in dfs.items():\n",
        "    # Ensure every desired column exists (create empty if missing)\n",
        "    for col in CANON_COLS:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "\n",
        "    dfs[year] = df.loc[:, CANON_COLS].copy()\n",
        "    print(f\"{year}: kept {dfs[year].shape[1]:>2} cols  →  {dfs[year].shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58RahvOAORPU",
        "outputId": "030ddf8b-d722-4448-f7f6-993a517e157d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017: kept 39 cols  →  (51392, 39)\n",
            "2018: kept 39 cols  →  (98855, 39)\n",
            "2019: kept 39 cols  →  (88883, 39)\n",
            "2020: kept 39 cols  →  (64461, 39)\n",
            "2021: kept 39 cols  →  (83439, 39)\n",
            "2022: kept 39 cols  →  (73268, 39)\n",
            "2023: kept 39 cols  →  (89184, 39)\n",
            "2024: kept 39 cols  →  (65437, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving the Cleaned DataFrames\n",
        "This part of the code is responsible for saving the cleaned dataframes back to your Google Drive. This is important for persisting the changes made to the data and for using the cleaned data in later analyses."
      ],
      "metadata": {
        "id": "jy8ND4zZVCzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# 4)  Save cleaned dataframes back to Drive\n",
        "# -------------------------------------------------------------------------\n",
        "CLEAN_DIR = os.path.join(DATA_DIR, \"clean_39cols\")\n",
        "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "for year, df in dfs.items():\n",
        "    out_path = os.path.join(CLEAN_DIR, f\"{year}_clean39.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n✅ All cleaned files written to: {CLEAN_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oaBucaFY-OP",
        "outputId": "7f3c3777-25d1-4f5c-e559-69db16723517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ All cleaned files written to: /content/drive/MyDrive/DataScienceLabProject/clean_39cols\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 3: Data Cleaning and Preparation\n",
        "This code block focuses on further cleaning the survey data and preparing it for analysis by:\n",
        "\n",
        "Dropping Empty Columns:\n",
        "It iterates through each year's data (dfs) and identifies columns that are completely empty (contain only NaN - Not a Number - values).\n",
        "Using dfs[year].isnull().all(), it checks if all values in a column are null.\n",
        "If a column is found to be empty, it's removed from the DataFrame for that year using dfs[year].drop(columns=empty_cols)."
      ],
      "metadata": {
        "id": "rLqxlFjbVIFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "# Cell 3 ────────────────────────────────────────────────────────────────────\n",
        "# Drop empty columns, low information columns, and convert to numeric\n",
        "\n",
        "# --- 1. Drop empty columns ---\n",
        "for year in dfs:\n",
        "    # Identify columns with all NaN values\n",
        "    empty_cols = dfs[year].columns[dfs[year].isnull().all()]\n",
        "    print(f\"Year {year}: Dropping {len(empty_cols)} completely empty columns\")\n",
        "    dfs[year] = dfs[year].drop(columns=empty_cols)\n",
        "\n",
        "# --- 2. Drop low information columns ---\n",
        "LOW_INFORMATION_THRESHOLD = 0.05  # 5%\n",
        "for year in dfs:\n",
        "    # Calculate the percentage of non-missing values per column\n",
        "    information_percentages = dfs[year].notna().sum() / len(dfs[year])\n",
        "\n",
        "    # Identify low-information columns\n",
        "    low_info_cols = information_percentages[\n",
        "        information_percentages < LOW_INFORMATION_THRESHOLD\n",
        "    ].index\n",
        "\n",
        "    print(f\"Year {year}: Dropping {len(low_info_cols)} low information columns\")\n",
        "    dfs[year] = dfs[year].drop(columns=low_info_cols)\n",
        "\n",
        "# --- 3. Convert to numeric ---\n",
        "for year in dfs:\n",
        "    # Select columns that are not 'respondent_id' or 'country'\n",
        "    numeric_cols = dfs[year].select_dtypes(exclude=['object']).columns\n",
        "    numeric_cols = numeric_cols[numeric_cols != 'respondent_id']\n",
        "    numeric_cols = numeric_cols[numeric_cols != 'country']\n",
        "\n",
        "    for col in numeric_cols:\n",
        "      try:\n",
        "        dfs[year][col] = pd.to_numeric(dfs[year][col], errors='coerce')\n",
        "      except Exception as e:\n",
        "        print(f\"Error converting {col} in {year}: {e}\")\n",
        "\n",
        "# --- 4. (Optional) Save cleaned dataframes back to Drive ---\n",
        "CLEAN_DIR = os.path.join(DATA_DIR, \"clean_numeric\")\n",
        "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "for year, df in dfs.items():\n",
        "    out_path = os.path.join(CLEAN_DIR, f\"{year}_clean_numeric.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n✅ All cleaned (numeric) files written to: {CLEAN_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8jlz4FpIAXH",
        "outputId": "332687cd-dcd3-4ef6-968a-374060145153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 2017: Dropping 11 completely empty columns\n",
            "Year 2018: Dropping 5 completely empty columns\n",
            "Year 2019: Dropping 0 completely empty columns\n",
            "Year 2020: Dropping 3 completely empty columns\n",
            "Year 2021: Dropping 7 completely empty columns\n",
            "Year 2022: Dropping 7 completely empty columns\n",
            "Year 2023: Dropping 9 completely empty columns\n",
            "Year 2024: Dropping 8 completely empty columns\n",
            "Year 2017: Dropping 1 low information columns\n",
            "Year 2018: Dropping 0 low information columns\n",
            "Year 2019: Dropping 0 low information columns\n",
            "Year 2020: Dropping 0 low information columns\n",
            "Year 2021: Dropping 0 low information columns\n",
            "Year 2022: Dropping 0 low information columns\n",
            "Year 2023: Dropping 0 low information columns\n",
            "Year 2024: Dropping 0 low information columns\n",
            "\n",
            "✅ All cleaned (numeric) files written to: /content/drive/MyDrive/DataScienceLabProject/clean_numeric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 4: Full Numerisation and One-Hot Encoding\n",
        "This cell aims to convert the remaining text-based or categorical data into numerical format, which is often necessary for machine learning algorithms. It uses two main techniques:\n",
        "\n",
        "Numerisation/Ordinal Mapping: Converting categories into ordered numbers (e.g., education levels: 'primary' → 8, 'bachelor' → 16, 'master' → 18, etc.).\n",
        "\n",
        "One-Hot Encoding: Creating new binary (0/1) columns for each category within multi-select fields (e.g., programming languages worked with: a column for Python, one for JavaScript, etc.)."
      ],
      "metadata": {
        "id": "ChWzgSMPVUty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔════════════════════════════════════════════════════════════════╗\n",
        "# ║  Efficient numerisation & one-hot for 2017-2024 SO surveys     ║\n",
        "# ╚════════════════════════════════════════════════════════════════╝\n",
        "import numpy as np, pandas as pd, re, os\n",
        "from collections import Counter\n",
        "\n",
        "TOP_N, RARE_THRESHOLD = 15, 30\n",
        "DATA_DIR = globals().get(\"DATA_DIR\", \"/content\")\n",
        "ENC_DIR  = f\"{DATA_DIR}/clean_numeric_encoded\"\n",
        "os.makedirs(ENC_DIR, exist_ok=True)\n",
        "\n",
        "# ── helper regex + dictionaries ──────────────────────────────────\n",
        "ORG_RE = re.compile(r\"(\\d+)[^\\d]+(\\d+)\")\n",
        "NUM_RE = re.compile(r\"\\d+(\\.\\d+)?\")          # for years_code\n",
        "EDU_MAP = {\n",
        "    \"primary\": 8, \"elementary\": 8, \"secondary\": 12, \"high school\": 12,\n",
        "    \"associate\": 14, \"bachelor\": 16, \"master\": 18,\n",
        "    \"professional\": 20, \"doctor\": 21,\n",
        "}\n",
        "SURVEY_LEN_MAP = {\"reasonable\": 1, \"too long\": 2, \"way too long\": 3}\n",
        "VISIT_MAP = {\"never\": 0, \"less\": 1, \"monthly\": 2, \"weekly\": 3,\n",
        "             \"daily\": 4, \"multiple\": 5}\n",
        "ACCOUNT_MAP, PART_MAP, EASE_MAP = {\"yes\": 1, \"no\": 0}, VISIT_MAP, {\n",
        "    \"very easy\": 5, \"easy\": 4, \"neither\": 3,\n",
        "    \"difficult\": 2, \"very difficult\": 1,\n",
        "}\n",
        "\n",
        "MULTI_COLS = [\n",
        "    \"dev_type\", \"langs_worked\", \"langs_desired\",\n",
        "    \"db_worked\", \"db_desired\",\n",
        "    \"platform_worked\", \"platform_desired\",\n",
        "    \"webframe_worked\", \"webframe_desired\",\n",
        "    \"misc_tech_worked\", \"misc_tech_desired\",\n",
        "    \"learning_resources\", \"job_factors\",\n",
        "    \"gender\", \"ethnicity\",\n",
        "]\n",
        "\n",
        "# ── 1) Build global token frequency (handles missing columns) ────\n",
        "concat = pd.concat(\n",
        "    [df.reindex(columns=MULTI_COLS) for df in dfs.values()],\n",
        "    axis=0, ignore_index=True\n",
        ")\n",
        "token_ctr = {\n",
        "    c: Counter(\n",
        "        tok.strip().lower()\n",
        "        for v in concat[c].dropna().astype(str)\n",
        "        for tok in v.split(\";\")\n",
        "    )\n",
        "    for c in MULTI_COLS\n",
        "}\n",
        "TOP_TOK = {c: [t for t, _ in ctr.most_common(TOP_N)] for c, ctr in token_ctr.items()}\n",
        "\n",
        "# ── 2) Fast converters ───────────────────────────────────────────\n",
        "def org_to_mid(s):\n",
        "    if pd.isna(s): return np.nan\n",
        "    st = str(s).lower()\n",
        "    if \"fewer\" in st: return float(re.search(r\"\\d+\", st)[0]) / 2\n",
        "    if \"more\"  in st or \"10000\" in st: return 2e4\n",
        "    m = ORG_RE.search(st)\n",
        "    return (int(m[1]) + int(m[2])) / 2 if m else np.nan\n",
        "\n",
        "def years_code(s):\n",
        "    if pd.isna(s): return np.nan\n",
        "    st = str(s).lower()\n",
        "    if \"less\" in st: return 0\n",
        "    if \"more\" in st: return 51\n",
        "    m = NUM_RE.search(st)\n",
        "    return float(m.group()) if m else np.nan\n",
        "\n",
        "def map_partial(series, mapper):\n",
        "    return series.str.lower().map(mapper)\n",
        "\n",
        "def onehot(df, col):\n",
        "    \"\"\"Return dummy columns for TOP_TOK[col], guaranteeing no duplicates.\"\"\"\n",
        "    if col not in df:\n",
        "        return pd.DataFrame(index=df.index)\n",
        "\n",
        "    # always treat the column as text\n",
        "    s = df[col].fillna(\"\").astype(str)\n",
        "\n",
        "    # split multi-select answers, lowercase & strip\n",
        "    d = s.str.get_dummies(\";\")\n",
        "    d.columns = d.columns.str.strip().str.lower()\n",
        "\n",
        "    # Collapse tokens that became identical after cleaning\n",
        "    if d.columns.duplicated().any():\n",
        "        d = d.T.groupby(level=0).max().T      # avoids the FutureWarning\n",
        "\n",
        "    # Ensure *every* top-N token exists (missing → 0)\n",
        "    d = d.reindex(columns=TOP_TOK[col], fill_value=0)\n",
        "\n",
        "    # Prefix for uniqueness across questions\n",
        "    d.columns = [f\"{col}__{c}\" for c in d.columns]\n",
        "    return d\n",
        "\n",
        "\n",
        "\n",
        "# ── 3) Encode every year ─────────────────────────────────────────\n",
        "encoded_dfs = {}\n",
        "for yr, df in dfs.items():\n",
        "    df = df.copy()\n",
        "\n",
        "    # numeric maps\n",
        "    for col in (\"years_code_total\", \"years_code_pro\"):\n",
        "        if col in df: df[col] = df[col].map(years_code)\n",
        "    if \"org_size\" in df: df[\"org_size\"] = df[\"org_size\"].map(org_to_mid)\n",
        "    if \"education_level\" in df:\n",
        "        df[\"education_level\"] = (\n",
        "            df[\"education_level\"].str.lower()\n",
        "            .apply(lambda x: next((v for k, v in EDU_MAP.items()\n",
        "                                   if pd.notna(x) and k in x), np.nan))\n",
        "        )\n",
        "    if \"survey_length\" in df:\n",
        "        df[\"survey_length\"] = map_partial(df[\"survey_length\"], SURVEY_LEN_MAP)\n",
        "    for col, mp in ((\"so_visit_freq\", VISIT_MAP), (\"so_part_freq\", PART_MAP),\n",
        "                    (\"so_account\", ACCOUNT_MAP), (\"survey_ease\", EASE_MAP)):\n",
        "        if col in df: df[col] = map_partial(df[col], mp)\n",
        "\n",
        "    # age band midpoint\n",
        "    if \"age\" in df and df[\"age\"].dtype == \"O\":\n",
        "        df[\"age\"] = pd.to_numeric(df[\"age\"].str.extract(r\"(\\d+)\")[0], errors=\"coerce\")\n",
        "\n",
        "    # multi-select one-hot\n",
        "    multi_df = pd.concat([onehot(df, c) for c in MULTI_COLS], axis=1)\n",
        "    multi_df = multi_df.loc[:, multi_df.sum() >= RARE_THRESHOLD]\n",
        "\n",
        "    # drop raw text cols & add dummies\n",
        "    df = df.drop(columns=[c for c in MULTI_COLS if c in df]).join(multi_df)\n",
        "\n",
        "    # encode residual object columns (except respondent_id / country)\n",
        "    EXEMPT_OBJ = {\"respondent_id\", \"country\", \"currency\"}  # ← added currency\n",
        "    for c in df.select_dtypes(\"object\"):\n",
        "      if c in EXEMPT_OBJ:\n",
        "          continue\n",
        "      df[c] = df[c].astype(\"category\").cat.codes.replace(-1, np.nan)\n",
        "\n",
        "\n",
        "    encoded_dfs[yr] = df\n",
        "    df.to_csv(f\"{ENC_DIR}/{yr}_encoded.csv\", index=False)\n",
        "    print(f\"{yr}: {df.shape}\")\n",
        "\n",
        "print(\"✅ Encoded CSVs in\", ENC_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FiM3aRqWLrx",
        "outputId": "f18f060a-7a86-46f8-9fbd-f1c1e51ebf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017: (51392, 83)\n",
            "2018: (98855, 124)\n",
            "2019: (88883, 172)\n",
            "2020: (64461, 168)\n",
            "2021: (83439, 151)\n",
            "2022: (73268, 162)\n",
            "2023: (89184, 145)\n",
            "2024: (65437, 146)\n",
            "✅ Encoded CSVs in /content/drive/MyDrive/DataScienceLabProject/clean_numeric_encoded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════╗\n",
        "# ║  Cell 4-bis : FX  &  CPI lookup dictionaries  ║\n",
        "# ╚══════════════════════════════════════════════╝\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1.  Average yearly FX to USD (mid-market) ---\n",
        "# Numbers below are illustrative.  Replace / extend as you like.\n",
        "FX_DICT = {\n",
        "    2017: {'USD':1, 'EUR':1.13, 'GBP':1.29, 'INR':0.015},\n",
        "    2018: {'USD':1, 'EUR':1.18, 'GBP':1.34, 'INR':0.014},\n",
        "    2019: {'USD':1, 'EUR':1.12, 'GBP':1.28, 'INR':0.014},\n",
        "    2020: {'USD':1, 'EUR':1.14, 'GBP':1.28, 'INR':0.013},\n",
        "    2021: {'USD':1, 'EUR':1.18, 'GBP':1.38, 'INR':0.013},\n",
        "    2022: {'USD':1, 'EUR':1.05, 'GBP':1.24, 'INR':0.012},\n",
        "    2023: {'USD':1, 'EUR':1.08, 'GBP':1.24, 'INR':0.012},\n",
        "    2024: {'USD':1, 'EUR':1.09, 'GBP':1.27, 'INR':0.012},\n",
        "}\n",
        "\n",
        "fx_rates = (\n",
        "    pd\n",
        "    .DataFrame([\n",
        "        {'year': y, 'currency_iso': cur, 'usd_rate': rate}\n",
        "        for y, mp in FX_DICT.items()\n",
        "        for cur, rate in mp.items()\n",
        "    ])\n",
        ")\n",
        "\n",
        "# --- 2.  CPI index (2024 = 100) ---\n",
        "CPI = {2017:86.0, 2018:88.1, 2019:89.7, 2020:91.6,\n",
        "       2021:94.0, 2022:97.1, 2023:98.9, 2024:100.0}\n",
        "\n",
        "cpi_factor = pd.Series(\n",
        "    {y: CPI[2024]/v for y, v in CPI.items()},\n",
        "    name='cpi_factor'\n",
        ")\n",
        "print(\"✅  FX & CPI lookups built in-memory\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5xFruRrPJ5u",
        "outputId": "eaa4c55e-3b12-4890-a98c-421c4c95d01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  FX & CPI lookups built in-memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 5 : add salary_usd_nom  &  salary_usd_2024 to each df  ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import re, numpy as np\n",
        "\n",
        "ISO_RE   = re.compile(r\"\\b[A-Z]{3}\\b\")\n",
        "SPECIALS = {\n",
        "    \"u.s. dollars\": \"USD\", \"us dollars\": \"USD\", \"u.s. dollar\": \"USD\",\n",
        "    \"euros\": \"EUR\", \"british pounds sterling\": \"GBP\",\n",
        "    \"south african rands\": \"ZAR\", \"canadian dollars\": \"CAD\",\n",
        "    \"australian dollars\": \"AUD\", \"polish złoty\": \"PLN\",\n",
        "    \"polish zloty\": \"PLN\", \"indian rupees\": \"INR\",\n",
        "    \"russian rubles\": \"RUB\", \"bitcoin\": \"BTC\",\n",
        "    \"non\": np.nan, \"none\": np.nan,\n",
        "}\n",
        "\n",
        "def to_iso(cur):\n",
        "    if pd.isna(cur):\n",
        "        return np.nan\n",
        "    s = str(cur).lower().strip()\n",
        "    for k, v in SPECIALS.items():\n",
        "        if k in s:\n",
        "            return v\n",
        "    m = ISO_RE.search(s.upper())   # ← use s here\n",
        "    return m.group() if m else np.nan\n",
        "\n",
        "\n",
        "for yr, df in encoded_dfs.items():\n",
        "    df = df.copy()\n",
        "\n",
        "    # a) currency → ISO\n",
        "    df[\"currency_iso\"] = df[\"currency\"].apply(to_iso)\n",
        "\n",
        "    # b) left-join FX for that year\n",
        "    fx_year = fx_rates.query(\"year == @yr\")\n",
        "    df = df.merge(fx_year, on=\"currency_iso\", how=\"left\")\n",
        "\n",
        "    # c) salary conversions\n",
        "    df[\"salary_usd_nom\"] = np.where(\n",
        "        df[\"currency_iso\"] == \"USD\",\n",
        "        df[\"compensation_total\"],                # already USD\n",
        "        df[\"compensation_total\"] * df[\"usd_rate\"]   # convert\n",
        "    )\n",
        "\n",
        "    df[\"salary_usd_2024\"] = df[\"salary_usd_nom\"] * cpi_factor.loc[yr]\n",
        "\n",
        "    # d) push back into dict (no writing to disk)\n",
        "    encoded_dfs[yr] = df\n",
        "    print(f\"{yr}:  added salary columns\")\n",
        "\n",
        "print(\"🎯  All years converted (nominal & real 2024 USD)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FawLzMnT7TDt",
        "outputId": "bddb8e62-a603-4701-9fd5-23a2066259e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017:  added salary columns\n",
            "2018:  added salary columns\n",
            "2019:  added salary columns\n",
            "2020:  added salary columns\n",
            "2021:  added salary columns\n",
            "2022:  added salary columns\n",
            "2023:  added salary columns\n",
            "2024:  added salary columns\n",
            "🎯  All years converted (nominal & real 2024 USD)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop empty columns from each year."
      ],
      "metadata": {
        "id": "pMQgsSg_ae_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Check if any encoded_dfs[year] has completely empty columns, if so, drop them\n",
        "\n",
        "for year in encoded_dfs:\n",
        "    # Identify columns with all NaN values\n",
        "    empty_cols = encoded_dfs[year].columns[encoded_dfs[year].isnull().all()]\n",
        "    if len(empty_cols) > 0:\n",
        "        print(f\"Year {year}: Dropping {len(empty_cols)} completely empty columns from encoded_dfs\")\n",
        "        encoded_dfs[year] = encoded_dfs[year].drop(columns=empty_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPJpYU8PaDzV",
        "outputId": "82fe882b-dac3-4e09-c66e-d9930e53176f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 2017: Dropping 1 completely empty columns from encoded_dfs\n",
            "Year 2018: Dropping 3 completely empty columns from encoded_dfs\n",
            "Year 2019: Dropping 2 completely empty columns from encoded_dfs\n",
            "Year 2020: Dropping 2 completely empty columns from encoded_dfs\n",
            "Year 2021: Dropping 2 completely empty columns from encoded_dfs\n",
            "Year 2022: Dropping 2 completely empty columns from encoded_dfs\n",
            "Year 2023: Dropping 2 completely empty columns from encoded_dfs\n",
            "Year 2024: Dropping 2 completely empty columns from encoded_dfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform Country to median salary by year"
      ],
      "metadata": {
        "id": "olLTwWhIajPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  run this in a new cell (only once per runtime)\n",
        "!pip install -q pycountry\n"
      ],
      "metadata": {
        "id": "gu_t20FKcRnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════╗\n",
        "# ║  Cell 6 : country-level median salary panel   ║\n",
        "# ╚══════════════════════════════════════════════╝\n",
        "import pandas as pd, requests, io, numpy as np, pycountry\n",
        "\n",
        "URL = \"https://ourworldindata.org/grapher/daily-median-income.csv\"\n",
        "med = pd.read_csv(io.BytesIO(requests.get(URL).content))\n",
        "\n",
        "# ── 1. keep survey years only ─────────────────────────────────────────\n",
        "med = med.query(\"Year >= 2017 & Year <= 2024\")\n",
        "\n",
        "# ── 2. identify the income-value column dynamically ───────────────────\n",
        "value_col = next(c for c in med.columns if c not in (\"Entity\", \"Code\", \"Year\"))\n",
        "med = med.rename(columns={\n",
        "    \"Entity\": \"country\",\n",
        "    \"Code\":   \"iso3\",\n",
        "    \"Year\":   \"year\",\n",
        "    value_col: \"med_ppp_2017\"          # daily PPP-$ (base-year 2017)\n",
        "})\n",
        "\n",
        "# ── 3. daily → annual, PPP-2017 → nominal-2024 USD  ───────────────────\n",
        "med[\"med_ppp_2017_year\"] = med[\"med_ppp_2017\"] * 365        # days → year\n",
        "cpi_ratio = CPI[2024] / CPI[2017]                           # from your Cell 4-bis\n",
        "med[\"med_usd_2024\"] = med[\"med_ppp_2017_year\"] * cpi_ratio  # rough PPP→nominal\n",
        "\n",
        "# quick sanity check\n",
        "print(med.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgGwNt_FaS0c",
        "outputId": "95e4fd90-1255-45e8-c200-d995a4e43d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    country iso3  year  med_ppp_2017 990177-annotations  med_ppp_2017_year  \\\n",
            "8   Albania  ALB  2017     10.187669                NaN        3718.499185   \n",
            "9   Albania  ALB  2018     11.639976                NaN        4248.591422   \n",
            "10  Albania  ALB  2019     11.611524                NaN        4238.206260   \n",
            "\n",
            "    med_usd_2024  \n",
            "8    4323.836262  \n",
            "9    4940.222584  \n",
            "10   4928.146814  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════╗\n",
        "# ║  Cell 7 : attach country median & compute ratios  ║\n",
        "# ╚══════════════════════════════════════════════════╝\n",
        "import pandas as pd, numpy as np\n",
        "from functools import lru_cache\n",
        "import pycountry        # ← already installed or pip-installed above\n",
        "\n",
        "@lru_cache\n",
        "def name_to_iso3(name):\n",
        "    \"\"\"convert free-text country → ISO-3; NaN → NaN\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return np.nan            # <-- skip missing entries\n",
        "    try:\n",
        "        return pycountry.countries.search_fuzzy(str(name))[0].alpha_3\n",
        "    except LookupError:\n",
        "        return np.nan            # <-- unknown spelling\n",
        "\n",
        "for yr, df in encoded_dfs.items():\n",
        "    df = df.copy()\n",
        "\n",
        "    # a) add ISO-3 code\n",
        "    df[\"iso3\"] = df[\"country\"].map(name_to_iso3)\n",
        "\n",
        "    # b) join the median-income table for this year\n",
        "    med_yr = med.query(\"year == @yr\")[[\"iso3\", \"med_usd_2024\"]]\n",
        "    df = df.merge(med_yr, on=\"iso3\", how=\"left\")\n",
        "\n",
        "    # c) new columns\n",
        "    df.rename(columns={\"med_usd_2024\": \"country_median_usd_2024\"}, inplace=True)\n",
        "    df[\"salary_vs_country_median\"] = (\n",
        "        df[\"salary_usd_2024\"] / df[\"country_median_usd_2024\"]\n",
        "    )\n",
        "\n",
        "    encoded_dfs[yr] = df\n",
        "    print(f\"{yr}: median added & ratio computed\")\n",
        "\n",
        "print(\"Done – use ‘salary_vs_country_median’ for downstream models\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK5plIIqbOyn",
        "outputId": "cf1839d0-00be-4bdc-dc53-1dfb761b306b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017: median added & ratio computed\n",
            "2018: median added & ratio computed\n",
            "2019: median added & ratio computed\n",
            "2020: median added & ratio computed\n",
            "2021: median added & ratio computed\n",
            "2022: median added & ratio computed\n",
            "2023: median added & ratio computed\n",
            "2024: median added & ratio computed\n",
            "Done – use ‘salary_vs_country_median’ for downstream models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dfs[2021][\n",
        "    [\"country\",\"salary_usd_2024\",\"country_median_usd_2024\",\"salary_vs_country_median\"]\n",
        "].head()\n"
      ],
      "metadata": {
        "id": "RRbialoycg50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5461acbd-2802-437d-bdf0-31cac7741412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             country  salary_usd_2024  \\\n",
              "0                                           Slovakia      6025.531915   \n",
              "1                                        Netherlands              NaN   \n",
              "2                                 Russian Federation              NaN   \n",
              "3                                            Austria              NaN   \n",
              "4  United Kingdom of Great Britain and Northern I...              NaN   \n",
              "\n",
              "   country_median_usd_2024  salary_vs_country_median  \n",
              "0              9964.628174                  0.604692  \n",
              "1             25348.954180                       NaN  \n",
              "2                      NaN                       NaN  \n",
              "3             24725.999767                       NaN  \n",
              "4             19892.547959                       NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01a79fa8-ce02-456a-9341-6d8d8c68c4da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>salary_usd_2024</th>\n",
              "      <th>country_median_usd_2024</th>\n",
              "      <th>salary_vs_country_median</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Slovakia</td>\n",
              "      <td>6025.531915</td>\n",
              "      <td>9964.628174</td>\n",
              "      <td>0.604692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Netherlands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25348.954180</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Russian Federation</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Austria</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24725.999767</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19892.547959</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01a79fa8-ce02-456a-9341-6d8d8c68c4da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01a79fa8-ce02-456a-9341-6d8d8c68c4da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01a79fa8-ce02-456a-9341-6d8d8c68c4da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3db480f8-1adc-432a-80df-f49aed1cc1ea\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3db480f8-1adc-432a-80df-f49aed1cc1ea')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3db480f8-1adc-432a-80df-f49aed1cc1ea button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Netherlands\",\n          \"United Kingdom of Great Britain and Northern Ireland\",\n          \"Russian Federation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salary_usd_2024\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 6025.531914893617,\n        \"max\": 6025.531914893617,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6025.531914893617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country_median_usd_2024\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7110.214873953012,\n        \"min\": 9964.628174418605,\n        \"max\": 25348.95418023256,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          25348.95418023256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salary_vs_country_median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6046920978308538,\n        \"max\": 0.6046920978308538,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6046920978308538\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop the country column from all years and display the head of some years\n",
        "\n",
        "for year in encoded_dfs:\n",
        "    if 'country' in encoded_dfs[year].columns:\n",
        "        encoded_dfs[year] = encoded_dfs[year].drop(columns=['country'])\n",
        "for year in encoded_dfs:\n",
        "    if 'currency' in encoded_dfs[year].columns:\n",
        "        encoded_dfs[year] = encoded_dfs[year].drop(columns=['currency'])\n",
        "\n",
        "# Verify that 'currency' column is dropped\n",
        "for year in encoded_dfs:\n",
        "    print(f\"Year {year}: 'currency' column exists: {'currency' in encoded_dfs[year].columns}\")\n",
        "\n",
        "for year in encoded_dfs:\n",
        "    if 'iso3' in encoded_dfs[year].columns:\n",
        "        encoded_dfs[year] = encoded_dfs[year].drop(columns=['iso3'])\n",
        "\n",
        "# Verify that 'iso3' column is dropped\n",
        "for year in encoded_dfs:\n",
        "    print(f\"Year {year}: 'iso3' column exists: {'iso3' in encoded_dfs[year].columns}\")\n",
        "\n",
        "for year in encoded_dfs:\n",
        "    if 'currency_iso' in encoded_dfs[year].columns:\n",
        "        encoded_dfs[year] = encoded_dfs[year].drop(columns=['currency_iso'])\n",
        "\n",
        "# Verify that 'currency_iso' column is dropped\n",
        "for year in encoded_dfs:\n",
        "    print(f\"Year {year}: 'currency_iso' column exists: {'currency_iso' in encoded_dfs[year].columns}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuDchdargjvb",
        "outputId": "fa95ad5a-feb6-475a-c479-33e8472e1201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 2017: 'currency' column exists: False\n",
            "Year 2018: 'currency' column exists: False\n",
            "Year 2019: 'currency' column exists: False\n",
            "Year 2020: 'currency' column exists: False\n",
            "Year 2021: 'currency' column exists: False\n",
            "Year 2022: 'currency' column exists: False\n",
            "Year 2023: 'currency' column exists: False\n",
            "Year 2024: 'currency' column exists: False\n",
            "Year 2017: 'iso3' column exists: False\n",
            "Year 2018: 'iso3' column exists: False\n",
            "Year 2019: 'iso3' column exists: False\n",
            "Year 2020: 'iso3' column exists: False\n",
            "Year 2021: 'iso3' column exists: False\n",
            "Year 2022: 'iso3' column exists: False\n",
            "Year 2023: 'iso3' column exists: False\n",
            "Year 2024: 'iso3' column exists: False\n",
            "Year 2017: 'currency_iso' column exists: False\n",
            "Year 2018: 'currency_iso' column exists: False\n",
            "Year 2019: 'currency_iso' column exists: False\n",
            "Year 2020: 'currency_iso' column exists: False\n",
            "Year 2021: 'currency_iso' column exists: False\n",
            "Year 2022: 'currency_iso' column exists: False\n",
            "Year 2023: 'currency_iso' column exists: False\n",
            "Year 2024: 'currency_iso' column exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n",
        "ABove we've transform the data to keep only common features present in 5 years or more ( out of 8 ) and transformed them to numeric"
      ],
      "metadata": {
        "id": "lE5BV6pln36k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll start doing some analysis of the data"
      ],
      "metadata": {
        "id": "SJJB3kwYoJk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 1 : build df_all  +  quick variable overview helper    ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# 1) stack yearly frames → one master DF\n",
        "df_all = (\n",
        "    pd.concat(\n",
        "        {yr: df for yr, df in encoded_dfs.items()},      # keep year key\n",
        "        names=[\"survey_year\"]\n",
        "    )\n",
        "    .reset_index(level=0)                                # bring year into column\n",
        ")\n",
        "\n",
        "# 2) helper → tidy variable summary\n",
        "def var_table(df: pd.DataFrame, top_n: int | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Return a table: variable, dtype, % missing, n_unique (optionally top_n).\"\"\"\n",
        "    tbl = (\n",
        "        pd.DataFrame({\n",
        "            \"dtype\":   df.dtypes.astype(str),\n",
        "            \"missing_%\": df.isna().mean() * 100,\n",
        "            \"n_unique\":  df.nunique(dropna=True)\n",
        "        })\n",
        "        .sort_values(\"missing_%\", ascending=False)\n",
        "    )\n",
        "    return tbl.head(top_n) if top_n else tbl\n",
        "\n",
        "# 3) quick sanity prints\n",
        "print(f\"✅ df_all shape: {df_all.shape[0]:,} rows × {df_all.shape[1]} columns\")\n",
        "print(var_table(df_all, top_n=5))\n"
      ],
      "metadata": {
        "id": "qtj7qSmVoIP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 2-bis : HARD-CODE columns to remove & drop them        ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# === 1) explicit list of survey-meta / open-text columns =======\n",
        "COLS_TO_REMOVE = [\n",
        "    # ---- gender & free-text minorities ----\n",
        "    \"gender__other\",\n",
        "    \"gender__gender non-conforming\",\n",
        "    \"gender__nonbinary, genderfluid, or gender non-conforming\",\n",
        "\n",
        "    # ---- open feedback / comments ----\n",
        "    \"survey_feedback\",\n",
        "    \"additional_comments\",\n",
        "    \"extra_feedback\",\n",
        "\n",
        "    # ---- long free-text questions ----\n",
        "    \"job_factors__languages, frameworks, and other tools\",\n",
        "    \"job_factors__other\",\n",
        "    \"dev_environment__other\",\n",
        "\n",
        "    # ---- sparse work–life perception questions ----\n",
        "    \"work_life_balance\",\n",
        "    \"work_life_balance__comment\",\n",
        "\n",
        "    # ---- ethnicity very-sparse dummies ----\n",
        "    \"ethnicity__indian\",\n",
        "    \"ethnicity__pakistani\",\n",
        "    \"ethnicity__other\",\n",
        "\n",
        "    # ---- any other free-text columns you flagged ----\n",
        "    # (add more names here if needed)\n",
        "]\n",
        "\n",
        "# === 2) drop from master DataFrame ==================================\n",
        "df_all.drop(columns=COLS_TO_REMOVE, inplace=True, errors=\"ignore\")\n",
        "\n",
        "# === 3) drop from every yearly DataFrame ============================\n",
        "for yr, df in encoded_dfs.items():\n",
        "    encoded_dfs[yr] = df.drop(columns=COLS_TO_REMOVE, errors=\"ignore\")\n",
        "\n",
        "# === 4) sanity report ===============================================\n",
        "print(f\"🚮  Dropped {len(COLS_TO_REMOVE)} columns.\")\n",
        "print(\"✅  df_all new shape:\", df_all.shape)\n"
      ],
      "metadata": {
        "id": "WgFhd7rlyfHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 2 : create & save full variable-overview               ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "# 1) build the full table\n",
        "var_overview = var_table(df_all)          # uses helper from Cell 1\n",
        "\n",
        "# 2) save for later inclusion in the PDF / appendix\n",
        "var_overview.to_csv(\"variable_overview.csv\")\n",
        "\n",
        "# 3) show a glimpse\n",
        "print(\"Top-10 variables by missing-%\")\n",
        "display(var_overview.head(10))\n",
        "print(\"\\n📝  Full table saved → variable_overview.csv\")\n"
      ],
      "metadata": {
        "id": "QYaGQEdZowfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files; files.download('variable_overview.csv')"
      ],
      "metadata": {
        "id": "gOvPbJC-pNx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 7 : filter salary outliers and create df_all_clean     ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# 1) look at extreme quantiles (for info only)\n",
        "q1, q99 = df_all[\"salary_usd_2024\"].quantile([0.01, 0.99])\n",
        "print(f\"1 % quantile ≈ {q1:,.0f}  |  99 % quantile ≈ {q99:,.0f}\")\n",
        "\n",
        "# 2) define a pragmatic band (adjust if needed)\n",
        "LOWER, UPPER = 1_000, 800_000    # USD (real 2024)\n",
        "\n",
        "# 3) build a clean copy\n",
        "mask_valid = df_all[\"salary_usd_2024\"].between(LOWER, UPPER, inclusive=\"both\")\n",
        "df_all_clean = df_all.loc[mask_valid].copy()\n",
        "\n",
        "# also regenerate encoded_dfs_clean if you need per-year\n",
        "encoded_dfs_clean = {\n",
        "    yr: df.loc[df[\"salary_usd_2024\"].between(LOWER, UPPER, inclusive=\"both\")].copy()\n",
        "    for yr, df in encoded_dfs.items()\n",
        "}\n",
        "\n",
        "# 4) report effect\n",
        "removed = (~mask_valid).sum()\n",
        "print(f\"🚮  Filtered out {removed:,} rows outside [{LOWER:,}, {UPPER:,}] USD.\")\n",
        "print(\"✅  df_all_clean shape:\", df_all_clean.shape)\n",
        "\n",
        "print(\"\\nClean salary stats:\")\n",
        "display(df_all_clean[\"salary_usd_2024\"].describe())\n"
      ],
      "metadata": {
        "id": "EgUfzHMlDecM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall salary distribution"
      ],
      "metadata": {
        "id": "NUszDWIQ5crU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 5 : overall salary_usd_2024 histogram                  ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "df_all[\"salary_usd_2024\"].dropna().plot.hist(bins=50)\n",
        "plt.xlabel(\"Salary (real 2024 USD)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Overall Distribution of Real 2024-USD Salaries\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dv37YXZL5P3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Year-by-year salary density"
      ],
      "metadata": {
        "id": "UoZt1esh5u12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  Cell 6 : salary_usd_2024 KDE – one figure per year          ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for yr in sorted(df_all[\"survey_year\"].unique()):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    df_all.query(\"survey_year == @yr\")[\"salary_usd_2024\"].dropna().plot.kde()\n",
        "    plt.xlabel(\"Salary (real 2024 USD)\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(f\"Salary Density – Survey {yr}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sQJs3yLK5wY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SwdmRNeA5xQk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-xoGn0FARuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1GMaIJSAn-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}